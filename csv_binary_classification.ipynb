{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_classif\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will load in the training and testing datasets, then identify features that are entirely absent in the test dataset and remove these columns from both datasets (because they are not useable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aggregated_scores_1', 'aggregated_scores_43', 'aggregated_scores_44', 'aggregated_scores_45', 'aggregated_scores_46', 'aggregated_scores_47', 'aggregated_scores_48', 'aggregated_scores_49', 'aggregated_scores_50', 'aggregated_scores_51', 'aggregated_scores_52', 'aggregated_scores_53', 'aggregated_scores_54', 'aggregated_scores_55', 'aggregated_scores_56', 'aggregated_scores_57', 'aggregated_scores_58', 'aggregated_scores_59', 'aggregated_scores_60', 'aggregated_scores_61']\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Identify columns that are absent in the test data\n",
    "missing_in_test = test_data.columns[test_data.isna().all()].tolist()\n",
    "\n",
    "# #Remove those columns from both datasets\n",
    "reduced_train_data = train_data.drop(columns=missing_in_test)\n",
    "reduced_test_data = test_data.drop(columns=missing_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reduced training dataframe has 278 feature columns and 276 instance rows.\n",
      "There are 3 feature columns that have missing values. Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>demographic_field_4</th>\n",
       "      <td>35</td>\n",
       "      <td>12.7</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demographic_field_5</th>\n",
       "      <td>35</td>\n",
       "      <td>12.7</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demographic_field_2</th>\n",
       "      <td>22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Missing Values  % of Total Values Data Type\n",
       "demographic_field_4              35               12.7   float64\n",
       "demographic_field_5              35               12.7   float64\n",
       "demographic_field_2              22                8.0   float64"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect features with missing values for some subejcts\n",
    "def missing_values_table(df, folds=False):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * mis_val / len(df)\n",
    "        mz_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mz_table = mz_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mz_table['Data Type'] = df.dtypes\n",
    "        mz_table = mz_table[mz_table.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n",
    "        if not folds==True:\n",
    "            print (\"The reduced training dataframe has \" + str(df.shape[1]) + \" feature columns and \" + \n",
    "                   str(df.shape[0]) + \" instance rows.\\n\"      \n",
    "                \"There are \" + str(mz_table.shape[0]) +\n",
    "                \" feature columns that have missing values. Summary:\")\n",
    "        \n",
    "        return mz_table\n",
    "\n",
    "instances_missing = missing_values_table(reduced_train_data, folds=False)\n",
    "instances_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demographic_field_4</th>\n",
       "      <th>demographic_field_5</th>\n",
       "      <th>demographic_field_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>241.000000</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>254.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.946058</td>\n",
       "      <td>0.788382</td>\n",
       "      <td>13.208661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.226373</td>\n",
       "      <td>0.409306</td>\n",
       "      <td>4.145544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       demographic_field_4  demographic_field_5  demographic_field_2\n",
       "count           241.000000           241.000000           254.000000\n",
       "mean              0.946058             0.788382            13.208661\n",
       "std               0.226373             0.409306             4.145544\n",
       "min               0.000000             0.000000             0.000000\n",
       "25%               1.000000             1.000000            11.000000\n",
       "50%               1.000000             1.000000            12.000000\n",
       "75%               1.000000             1.000000            16.000000\n",
       "max               1.000000             1.000000            30.000000"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_train_data[list(instances_missing.index)].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values comprise a small proportion of the data (more than 25-30% might be problematic). Will aim to replace missing values with the mean of their column (for continuous feature) or the mode (for binary features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_subset = reduced_train_data \n",
    "training_ids = training_subset.iloc[:, [0]]\n",
    "# draw_histograms(training_subset, training_subset.columns[1:21], 5, 4)\n",
    "\n",
    "testing_subset = reduced_test_data \n",
    "testing_ids = testing_subset.iloc[:,[0]]\n",
    "\n",
    "# Extract instance classification labels from the training dataset\n",
    "classification_labels = training_subset.iloc[:, [-1]]\n",
    "training_subset = training_subset.drop(columns=[\"id\", \"label\"])\n",
    "testing_subset = testing_subset.drop(columns=\"id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include plotting function for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot individual feature distributions  as histograms\n",
    "colors = list(mcolors.CSS4_COLORS.keys())[10:]\n",
    "def draw_histograms(dataframe, features, rows, cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, feature in enumerate(features):\n",
    "        ax=fig.add_subplot(rows,cols,i+1)\n",
    "        dataframe[feature].hist(bins=20,ax=ax,facecolor=colors[i])\n",
    "        ax.set_title(feature+\" Histogram\",color=colors[35])\n",
    "#         ax.set_yscale('log')\n",
    "    fig.tight_layout() \n",
    "    #plt.savefig('Histograms.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Consider correlation of features\n",
    "#     plt.figure(figsize = (38,16))\n",
    "#     sns.heatmap(fold_train_selected.corr(), annot = True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise feature data using min-max scaling - this will scale all feature values to the range 0-1. This will not affect the values of binary features. Standardization may be preferable for Guassian distributed continuous features, however, this dataset includes many binary features and some non-Gaussian continuous features.\n",
    "\n",
    "Could consider normalisation for some features and standardisation for others (future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_train_test(training_subset, testing_subset):\n",
    "    # fit scaler on training data\n",
    "    norm = MinMaxScaler().fit(training_subset)\n",
    "\n",
    "    # transform training data\n",
    "    normalised_training_subset = training_subset.copy(deep=True)\n",
    "    normalised_training_subset[normalised_training_subset.columns] = norm.transform(training_subset[training_subset.columns])\n",
    "\n",
    "    # transform testing data\n",
    "    # Testing/validation data is normalised using training data feature ranges \n",
    "    # -> min and max should not be exactly 0 and 1\n",
    "    normalised_testing_subset = testing_subset.copy(deep=True)\n",
    "    normalised_testing_subset[normalised_testing_subset.columns] = norm.transform(testing_subset[testing_subset.columns])\n",
    "    \n",
    "    return normalised_training_subset, normalised_testing_subset\n",
    "\n",
    "# normalised_testing_subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with numerical input data (including some binary variables, assumed to be dummy variables in place of categorical data). As such, feature selection will be performed using a filter-based technique with the ANOVA correlation coefficient.\n",
    "- Not sure how far to reduce the feature set - is k<1/10th the no of samples reasonable?\n",
    "- This feature selection method will not take feature correlations into account because each feature is considered separately - need to include separate consideration of feature-feature correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_features(normalised_training_subset, training_labels, k_features):\n",
    "    # define feature selection\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features)\n",
    "\n",
    "    # apply feature selection\n",
    "    selector.fit(normalised_training_subset, np.squeeze(training_labels))\n",
    "    cols = selector.get_support(indices=True)\n",
    "\n",
    "    return cols\n",
    "\n",
    "def trim_correlated(df_in, threshold):\n",
    "    df_corr = df_in.corr(method='pearson')\n",
    "    df_not_correlated = ~(df_corr.mask(np.tril(np.ones([len(df_corr)]*2, dtype=bool))).abs() > threshold).any()\n",
    "    un_corr_idx = df_not_correlated.loc[df_not_correlated[df_not_correlated.index] == True].index\n",
    "#     df_out = df_in[un_corr_idx]\n",
    "    return un_corr_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model feature selection and training with 3-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features removed due to correlation 15\n",
      "Number of features removed due to correlation 18\n",
      "Number of features removed due to correlation 18\n",
      "Mean svc accuracy score over  3 folds is: 0.7645\n",
      "Mean LRC accuracy score over  3 folds is: 0.7790\n",
      "Mean KNNC accuracy score over  3 folds is: 0.7609\n"
     ]
    }
   ],
   "source": [
    "# prepare cross validation - 3 folds\n",
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# separate splits\n",
    "count = 0\n",
    "LRC_validation_accuracy = []\n",
    "svc_validation_accuracy = []\n",
    "KNNC_validation_accuracy = []\n",
    "for train, valid in kfold.split(training_subset):\n",
    "    \n",
    "    # separate data into train and test folds\n",
    "    fold_train = training_subset.iloc[train].copy(deep=True)\n",
    "    fold_train_labels = classification_labels.iloc[train].copy(deep=True)\n",
    "    fold_valid = training_subset.iloc[valid].copy(deep=True)\n",
    "    fold_valid_labels = classification_labels.iloc[valid].copy(deep=True)\n",
    "    \n",
    "    # Fill in missing data - determine if feature is binary or continuous\n",
    "    # Fill binary using the mode, fill continuous using the mean\n",
    "    fold_train_instances_missing = fold_train.columns[fold_train.isna().any()].tolist()\n",
    "    for iterant in fold_train_instances_missing:\n",
    "        if fold_train[iterant].max() == 1.000000:\n",
    "            fold_train[iterant].fillna(value = fold_train[iterant].mode()[0], inplace=True)\n",
    "        else:\n",
    "            fold_train[iterant].fillna(value = fold_train[iterant].mean(), inplace=True)\n",
    "            \n",
    "    fold_valid_instances_missing = fold_valid.columns[fold_valid.isna().any()].tolist()\n",
    "    for iterant in fold_valid_instances_missing:\n",
    "        if fold_valid[iterant].max() == 1.000000:\n",
    "            fold_valid[iterant].fillna(value = fold_valid[iterant].mode()[0], inplace=True)\n",
    "        else:\n",
    "            fold_valid[iterant].fillna(value = fold_valid[iterant].mean(), inplace=True)\n",
    "    \n",
    "    # Normalise features using MinMax Scaling\n",
    "    fold_norm_train, fold_norm_valid = normalise_train_test(fold_train, fold_valid)\n",
    "    \n",
    "#     # Inspect remaining feature correlations\n",
    "#     plt.figure(figsize = (38,16))\n",
    "#     sns.heatmap(fold_norm_train.iloc[:,0:20].corr(), annot = True)\n",
    "#     plt.show()\n",
    "    \n",
    "    # Remove features that are highly correlated with another selected feature\n",
    "    un_corr_idx = trim_correlated(fold_norm_train, 0.9)\n",
    "    print('Number of features removed due to correlation {0:2d}'. format((fold_norm_train.shape[1] - len(un_corr_idx))))\n",
    "    fold_train_uncorr = fold_norm_train[un_corr_idx]\n",
    "    fold_valid_uncorr = fold_norm_valid[un_corr_idx]\n",
    "    \n",
    "    # Run feature selection\n",
    "    feature_aim = 15\n",
    "    fold_cols_selected = select_features(fold_train_uncorr, fold_train_labels, feature_aim)\n",
    "    fold_train_selected = fold_train_uncorr.iloc[:,fold_cols_selected]\n",
    "    fold_valid_selected = fold_valid_uncorr.iloc[:,fold_cols_selected]\n",
    "#     print(fold_train_selected)\n",
    "#     draw_histograms(fold_train_selected,fold_train_selected.columns,8,4)\n",
    "    \n",
    "    ### Model options: this would be nicer in a function\n",
    "    \n",
    "    # Setup and fit SVM classifier\n",
    "    svc=SVC(C=1.0) \n",
    "    # fit classifier to training set\n",
    "    svc.fit(fold_train_selected, np.squeeze(fold_train_labels))\n",
    "    # make predictions on fold validation set\n",
    "    validation_predictions_svc=svc.predict(fold_valid_selected)\n",
    "    svc_accuracy = accuracy_score(np.squeeze(fold_valid_labels), validation_predictions_svc)\n",
    "    svc_validation_accuracy.append(svc_accuracy)\n",
    "    \n",
    "    # Setup and fit logistic regression classifier\n",
    "    LRC = LogisticRegression()\n",
    "    LRC.fit(fold_train_selected, np.squeeze(fold_train_labels))\n",
    "    # make predictions on fold validation set\n",
    "    validation_predictions_LRC=LRC.predict(fold_valid_selected)\n",
    "    LRC_accuracy = accuracy_score(np.squeeze(fold_valid_labels), validation_predictions_LRC)\n",
    "    LRC_validation_accuracy.append(LRC_accuracy)\n",
    "    \n",
    "    # Setup and fit K-nearest neighbour classifer\n",
    "    KNNC = KNeighborsClassifier(n_neighbors = 12)\n",
    "    KNNC.fit(fold_train_selected, np.squeeze(fold_train_labels))\n",
    "    validation_predictions_KNNC=KNNC.predict(fold_valid_selected)\n",
    "    KNNC_accuracy = accuracy_score(np.squeeze(fold_valid_labels), validation_predictions_KNNC)\n",
    "    KNNC_validation_accuracy.append(KNNC_accuracy)\n",
    "\n",
    "    count += 1\n",
    "    # Print validation report for each fold\n",
    "#     print('Fold {0:2d} prediction accuracy is: {1:0.4f}'. format(count, LRC_accuracy))\n",
    "#     print(classification_report(np.squeeze(fold_valid_labels), validation_predictions_svc))\n",
    "#     print(classification_report(np.squeeze(fold_valid_labels), validation_predictions_LRC))\n",
    "#     print(classification_report(np.squeeze(fold_valid_labels), validation_predictions_KNNC))\n",
    "\n",
    "print('Mean svc accuracy score over {0:2d} folds is: {1:0.4f}'. format(count, np.mean(np.asarray(svc_validation_accuracy))))\n",
    "print('Mean LRC accuracy score over {0:2d} folds is: {1:0.4f}'. format(count, np.mean(np.asarray(LRC_validation_accuracy))))\n",
    "print('Mean KNNC accuracy score over {0:2d} folds is: {1:0.4f}'. format(count, np.mean(np.asarray(KNNC_validation_accuracy))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector classifier, logistic regression classifier and K-nearest neighbour classifiers all provide comparable prediction accuracy over 5-fold cross validation, although the KNN's performance is slightly worse (and the classifer's performance is very sensitive to K, and variable across folds).\n",
    "\n",
    "Selecting the logistic regression classifier for testing on the unseen dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on full dataset and testing on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that model selection is complete, the full training dataset:\n",
    "- will have missing values filled (with feature mean for continuous features and feature mode for binary features,\n",
    "- will be normalised using MinMax scaling,\n",
    "- and will have highly correlated features removed.\n",
    "\n",
    "Filter based feature selection will then be performed to reduce the number of features to 15. \n",
    "\n",
    "Finally, a logistic regression classifier will be trained on the full training dataset and then used to predict classification labels for the full testing dataset.\n",
    "\n",
    "Questionable assumptions of the logistic regression classifier are:\n",
    "- that the observations are independent i.e. not repeat tests.\n",
    "- that there is little multicollinearity (feature-feature correlation) - this has been mitigated to some extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features removed due to correlation 15\n"
     ]
    }
   ],
   "source": [
    "# Fill in missing data - determine if feature is binary or continuous\n",
    "# Fill binary using the mode, fill continuous using the mean\n",
    "train_instances_missing = training_subset.columns[training_subset.isna().any()].tolist()\n",
    "for iterant in train_instances_missing:\n",
    "    if training_subset[iterant].max() == 1.000000:\n",
    "        training_subset[iterant].fillna(value = training_subset[iterant].mode()[0], inplace=True)\n",
    "    else:\n",
    "        training_subset[iterant].fillna(value = training_subset[iterant].mean(), inplace=True)\n",
    "        \n",
    "\n",
    "# Fill missing data as above for test dataset            \n",
    "test_instances_missing = testing_subset.columns[testing_subset.isna().any()].tolist()\n",
    "for iterant in test_instances_missing:\n",
    "    if testing_subset[iterant].max() == 1.000000:\n",
    "        testing_subset[iterant].fillna(value = testing_subset[iterant].mode()[0], inplace=True)\n",
    "    else:\n",
    "        testing_subset[iterant].fillna(value = testing_subset[iterant].mean(), inplace=True)\n",
    "    \n",
    "# Normalise features in training and test sets using MinMax Scaling\n",
    "norm_train, norm_test = normalise_train_test(training_subset, testing_subset)\n",
    "    \n",
    "# Remove features that are highly correlated with another selected feature\n",
    "un_corr_idx = trim_correlated(norm_train, 0.9)\n",
    "print('Number of features removed due to correlation {0:2d}'. format((fold_norm_train.shape[1] - len(un_corr_idx))))\n",
    "train_uncorr = norm_train[un_corr_idx]\n",
    "test_uncorr = norm_test[un_corr_idx]\n",
    "    \n",
    "# Run feature selection\n",
    "feature_aim = 15\n",
    "cols_selected = select_features(train_uncorr, classification_labels, feature_aim)\n",
    "train_selected = train_uncorr.iloc[:,cols_selected]\n",
    "test_selected = test_uncorr.iloc[:,cols_selected]\n",
    "\n",
    "# Setup and fit logistic regression classifier using full training dataset\n",
    "LRC = LogisticRegression()\n",
    "LRC.fit(train_selected, np.squeeze(classification_labels))\n",
    "\n",
    "# make predictions on test set\n",
    "testing_predictions_LRC=pd.DataFrame(data=LRC.predict(test_selected), columns=['label'])\n",
    "result = testing_ids.join(testing_predictions_LRC)\n",
    "# Save predictions to CSV file for sharing\n",
    "result.to_csv('test_labels.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
